# 梯度下降法
## 介紹
&emsp;&emsp;梯度下降法是一個一階最佳化算法，通常也稱為最陡下降法，但是不該與近似積分的最陡下降法（英語：Method of steepest descent）混淆。 要使用梯度下降法找到一個函數的局部極小值，必須向函數上當前點對應梯度（或者是近似梯度）的反方向的規定步長距離點進行疊代搜索。如果相反地向梯度正方向疊代進行搜索，則會接近函數的局部極大值點；這個過程則被稱為梯度上升法。

&emsp;&emsp;在線性規劃的歷史發展過程中所衍伸出的諸多概念，建立了最優化理論的核心思維，例如「對偶」、「分解」、「凸集」的重要性及其一般化等。

## 梯度下降法

* 梯度就是斜率最大的那個方向，所以梯度下降法，其實就是朝著斜率最大的方向走。梯度下降法，就是朝著《逆梯度》的方向走，於是就可以不斷下降，直到到達梯度為 0 的點 (斜率最大的方向仍然是斜率為零)，此時就已經到了一個《谷底》，也就是區域的最低點了！

![Pic](https://github.com/yuanxiii/ai110b/blob/main/note/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/b.jpg)

當要用程式計算梯度時，我們可以從梯度的定義中看到他的基本元素，也就是「偏微分」。

因此我們先用程式寫出計算偏微分的程式:

```
def df(f, p, k, step=0.01):
    p1 = p.copy()
    p1[k] = p[k]+step
    return (f(p1) - f(p)) / step
 
利用以下指令就能夠計算出 f(x,y) 在 (1,1) 這點的偏導數
p = [1.0, 1.0]
print('nn.df(f, p, 0) = ', nn.df(f, p, 0))	
```

能夠算偏微分之後，我們就能夠利用npGradient.py算出梯度了 在學會計算梯度後，就可以開始使用梯度下降法了!

* 下圖為xx + yy的圖形:

![Pic](https://github.com/yuanxiii/ai110b/blob/main/note/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/a.png)

在執行程式後可以看到他最後找到了(0,0)這個點

```
1:p=[1.0, 3.0] f(p)=10.000 gp=[2.009999999999934, 6.009999999999849] glen=6.33721
2:p=[0.9799 2.9399] f(p)=9.603 gp=[1.9698 5.8898] glen=6.21046
3:p=[0.960202 2.881002] f(p)=9.222 gp=[1.930404 5.772004] glen=6.08625
4:p=[0.94089796 2.82328196] f(p)=8.856 gp=[1.89179592 5.65656392] glen=5.96453
5:p=[0.92198    2.76671632] f(p)=8.505 gp=[1.85396    5.54343264] glen=5.84524
(中間省略)
658:p=[-0.00499827 -0.00499483] f(p)=0.000 gp=[3.45722613e-06 1.03372781e-05] glen=0.00001
659:p=[-0.00499831 -0.00499493] f(p)=0.000 gp=[3.38808161e-06 1.01305326e-05] glen=0.00001
660:p=[-0.00499834 -0.00499504] f(p)=0.000 gp=[3.32031998e-06 9.92792193e-06] glen=0.00001
661:p=[-0.00499837 -0.00499514] f(p)=0.000 gp=[3.25391358e-06 9.72936349e-06] glen=0.00001
662:p=[-0.00499841 -0.00499523] f(p)=0.000 gp=[3.18883531e-06 9.53477622e-06] glen=0.00001
663:p=[-0.00499844 -0.00499533] f(p)=0.000 gp=[3.1250586e-06 9.3440807e-06] glen=0.00001
```

## 缺點
>1. 靠近局部極小值時速度減慢。
>2. 直線搜索可能會產生一些問題。
>3. 可能會「之字型」地下降。

## 參考資料
>1. [https://wiki.mbalib.com/zh-tw/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92](https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95)
>2. https://www.youtube.com/watch?v=fegAeph9UaA&list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&index=4
>3. https://kinmen6.com/root/%E9%99%B3%E9%8D%BE%E8%AA%A0/%E8%AA%B2%E7%A8%8B/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7/07-neural/02-gradient/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.md
>4. 百度百科
